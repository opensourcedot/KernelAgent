{{ triton_guidelines }}

TASK: Generate a complete Python implementation of a Triton kernel based on the following test requirements.

TEST CODE:
Analyze this test to understand what the kernel needs to implement:

```python
{{ test_code }}
```

CRITICAL REQUIREMENTS - NO CHEATING ALLOWED:
1. The implementation must be a complete, valid Python file
2. Import necessary modules (triton, triton.language as tl, torch)
3. Create a Triton kernel (can be named anything, e.g., _kernel) decorated with @triton.jit
4. MUST create a wrapper function named 'kernel_function' that:
   - Takes normal Python/PyTorch arguments (tensors, scalars, etc.)
   - Handles grid calculation and kernel launch internally
   - Returns results as normal Python/PyTorch objects
   - This wrapper is what the test will import and call

**STRICTLY FORBIDDEN - DO NOT CHEAT:**
- DO NOT call PyTorch functions (torch.add, torch.mul, torch.sum, etc.) inside the kernel computation
- DO NOT use PyTorch operations to perform the actual computation and just return the result
- DO NOT implement the logic using pure PyTorch and avoid writing Triton kernel code
- The actual computation MUST be implemented using Triton operations (tl.load, tl.store, tl.sum, etc.)
- The wrapper function can use PyTorch for tensor creation, memory allocation, and result formatting, but the core computation MUST happen in the Triton kernel

**WHAT IS REQUIRED:**
- Write actual Triton kernel code using triton.language operations
- Use tl.load() and tl.store() for memory access
- Use tl.sum(), tl.max(), tl.min() etc. for reductions  
- Use proper Triton indexing with tl.program_id(), tl.arange(), etc.
- Implement the algorithm step-by-step in Triton operations, not PyTorch shortcuts

5. Follow all Triton programming guidelines above
6. Include appropriate docstrings and comments
7. Handle edge cases and boundary conditions properly

Example structure (wrapper can use PyTorch, but kernel must use Triton):
```python
import triton
import triton.language as tl
import torch

@triton.jit
def _actual_kernel(ptr_a, ptr_b, ptr_out, n_elements, BLOCK_SIZE: tl.constexpr):
    # MUST use Triton operations here, NOT PyTorch
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load using Triton
    a = tl.load(ptr_a + offsets, mask=mask)
    b = tl.load(ptr_b + offsets, mask=mask)
    
    # Compute using Triton operations (NOT torch.add!)
    result = a + b
    
    # Store using Triton
    tl.store(ptr_out + offsets, result, mask=mask)

def kernel_function(tensor_a, tensor_b):
    """Wrapper function that handles kernel launch."""
    # PyTorch operations allowed here for setup
    output = torch.empty_like(tensor_a)
    n_elements = tensor_a.numel()
    
    # Calculate grid and launch Triton kernel
    BLOCK_SIZE = 1024
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    _actual_kernel[grid](
        tensor_a, tensor_b, output, n_elements, BLOCK_SIZE
    )
    return output
```

Generate a complete kernel implementation with actual Triton code: 